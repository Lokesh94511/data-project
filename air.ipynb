{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lokesh94511/data-project/blob/main/air.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "xn-0O2G7qfNV",
        "outputId": "f95209fc-f348-46e8-b765-02acedbc5caa"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'PM_test.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6e24de932d4a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PM_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PM_truth.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PM_test.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dropout, Dense\n",
        "import keras\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "df_test= pd.read_csv(\"PM_test.csv\")\n",
        "df_truth=pd.read_csv(\"PM_truth.csv\")\n",
        "df_train.head()\n",
        "df_test.head()\n",
        "df_truth.head()\n",
        "for i in range(1, 101):\n",
        "    max_rul = df_train[df_train['id'] == i]['cycle'].max()\n",
        "    df_train.loc[df_train['id'] == i, 'RUL'] = df_train[df_train['id'] == i]['cycle'].apply(lambda x: max_rul - x)\n",
        "\n",
        "# Define window values\n",
        "w0, w1 = 15, 30\n",
        "\n",
        "# Create label1 for training data\n",
        "df_train['label1'] = np.where(df_train['RUL'] <= w1, 1, 0)\n",
        "\n",
        "# Create label2 for training data\n",
        "df_train['label2'] = np.where(df_train['RUL'] > w1,0,\n",
        "                             np.where((df_train['RUL'] <= w1) & (df_train['RUL'] > w0),\n",
        "                                      1, 2))\n",
        "df_train.head()\n",
        "def normalize_data(df, col_not_to_norm):\n",
        "    columns_to_normalize = df.columns.difference(col_not_to_norm)\n",
        "\n",
        "    # Separate the columns\n",
        "    df_to_normalize = df[columns_to_normalize]\n",
        "    df_not_to_normalize = df[col_not_to_norm]\n",
        "\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    df_test['label2'] = np.where(df_test['RUL'] > w1, 0,\n",
        "                               np.where((df_test['RUL'] <= w1) & (df_test['RUL'] > w0),\n",
        "                                        1, 2))\n",
        "\n",
        "\n",
        "# Normalize data\n",
        "df_test = normalize_data(df_test, columns_not_to_normalize)\n",
        "df_test.head()\n",
        "# Select the engine ID\n",
        "engine_id = 1\n",
        "\n",
        "# Filter the dataframe for the selected engine\n",
        "engine_data = df_train[df_train['id'] == engine_id]\n",
        "\n",
        "# List of sensors to plot\n",
        "sensors = [f's{i}' for i in range(1, 22)]\n",
        "\n",
        "# Plot each sensor reading over cycles in separate graphs\n",
        "for sensor in sensors:\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    plt.plot(engine_data['cycle'], engine_data[sensor], label=sensor)\n",
        "    plt.xlabel('Cycle')\n",
        "    plt.ylabel('Sensor Reading')\n",
        "    plt.title(f'Sensor {sensor} Readings Over Cycles for Engine 1')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_train.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(20, 14))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=.5)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Correlation Heatmap of Sensor Readings and Settings')\n",
        "plt.xlabel('Sensors and Settings')\n",
        "plt.ylabel('Sensors and Settings')\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 50\n",
        "\n",
        "# function to reshape features into (samples, time steps, features)\n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_array[start:stop, :] # Added indentation here\n",
        "# pick the feature columns\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3']\n",
        "key_cols = ['id', 'cycle']\n",
        "label_cols = ['label1', 'label2', 'RUL']\n",
        "\n",
        "input_features = df_test.columns.values.tolist()\n",
        "sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
        "sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
        "sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
        "\n",
        "# The time is sequenced along\n",
        "# This may be a silly way to get these column names, but it's relatively clear\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "print(sequence_cols)\n",
        "# generator for the sequences\n",
        "seq_gen = (list(gen_sequence(df_train[df_train['id']==id], sequence_length, sequence_cols))\n",
        "           for id in df_train['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "seq_array.shape\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_array = id_df[label].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    return data_array[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(df_train[df_train['id']==id], sequence_length, ['label1'])\n",
        "             for id in df_train['id'].unique()]\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape\n",
        "\n",
        "\n",
        "# build the network\n",
        "# Feature weights\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "# GRU model\n",
        "model = Sequential()\n",
        "\n",
        "# The first layer\n",
        "model.add(GRU(\n",
        "         input_shape=(sequence_length, nb_features),\n",
        "         units=100,\n",
        "         return_sequences=True))\n",
        "# Plus a 20% dropout rate\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "# The second layer\n",
        "model.add(GRU(\n",
        "          units=50,\n",
        "          return_sequences=False))\n",
        "\n",
        "# Plus a 20% dropout rate\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Dense sigmoid layer\n",
        "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
        "\n",
        "# With adam optimizer and a binary crossentropy loss. We will optimize for model accuracy.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Verify the architecture\n",
        "print(model.summary())\n",
        "# Fit the network\n",
        "model.fit(seq_array, # Training features\n",
        "          label_array, # Training labels\n",
        "          epochs=10,   # We'll stop after 10 epochs\n",
        "          batch_size=200, #\n",
        "          validation_split=0.10, # Use 10% of data to evaluate the loss. (val_loss)\n",
        "          verbose=1, #\n",
        "          callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', # Monitor the validation loss\n",
        "                                                   min_delta=0,    # until it doesn't change (or gets worse)\n",
        "                                                   patience=5,  # patience > 1 so it continues if it is not consistently improving\n",
        "                                                   verbose=0,\n",
        "                                                   mode='auto')])\n",
        "# training metrics\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('Training Accurracy: {}'.format(scores[1]))\n",
        "# Make predictions\n",
        "y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\")\n",
        "y_true = label_array\n",
        "\n",
        "#Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "plt.xlabel('True Labels')\n",
        "plt.ylabel('Predicted Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "print('Training Precision: ', precision)\n",
        "print('Training Recall: ', recall)\n",
        "print('Training F1 Score:', f1)\n",
        "# Select the last sequence for each engine ID in the test data\n",
        "seq_array_test_last = [\n",
        "    df_test[df_test['id'] == id][sequence_cols].values[-sequence_length:]\n",
        "    for id in df_test['id'].unique() if len(df_test[df_test['id'] == id]) >= sequence_length\n",
        "]\n",
        "\n",
        "# Convert the list to a numpy array and ensure the data type is float32\n",
        "seq_array_test_last = np.array(seq_array_test_last, dtype=np.float32)\n",
        "seq_array_test_last.shape\n",
        "# Create a mask to filter engine IDs with enough data points\n",
        "y_mask = [len(df_test[df_test['id'] == id]) >= sequence_length for id in df_test['id'].unique()]\n",
        "\n",
        "# Extract the last label for each engine ID that meets the sequence length requirement\n",
        "label_array_test_last = df_test.groupby('id')['label1'].nth(-1)[y_mask].values\n",
        "\n",
        "# Reshape and convert to float32\n",
        "label_array_test_last = label_array_test_last.reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "# Display shapes of the test sequences and labels\n",
        "print(seq_array_test_last.shape)\n",
        "print(label_array_test_last.shape)\n",
        "# Evaluate the model on the test data\n",
        "test_scores = model.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "test_accuracy = test_scores[1]\n",
        "\n",
        "# Print and log the test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "# Make predictions\n",
        "y_test_pred = (model.predict(seq_array_test_last) > 0.5).astype(\"int32\")\n",
        "y_test_true = label_array_test_last\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "plt.xlabel('True Labels')\n",
        "plt.ylabel('Predicted Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(y_test_true, y_test_pred)\n",
        "recall = recall_score(y_test_true, y_test_pred)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print('Training Precision: ', precision)\n",
        "print('Training Recall: ', recall)\n",
        "print('Training F1 Score:', f1)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJyo5WBiY7U5SokbnugugK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}